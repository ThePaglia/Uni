{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Fqs1bZ3q3NKi"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m layers, regularizers\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB19h5MbMUap"
      },
      "source": [
        "# Predizione della qualità di un vino partendo da risultati di test fisico-chimici\n",
        "Il dataset è disponibile pubblicamente online (https://archive.ics.uci.edu/ml/datasets/Wine+Quality) e viene fornito (per rapidità/semplicità) in formato numpy (.npz) per quest'esercitazione (winequality-white.npz e winequality-red.npz). \n",
        "\n",
        "La matrice fornita nella variabile x ha dimensioni Nobs x 11 (vini bianchi: Nobs=4898; vini rossi: Nobs=1599). Le righe rappresentano le diverse osservazioni, mentre le colonne rappresentano le feature:\n",
        "```\n",
        "1 - fixed acidity\n",
        "2 - volatile acidity\n",
        "3 - citric acid\n",
        "4 - residual sugar\n",
        "5 - chlorides\n",
        "6 - free sulfur dioxide\n",
        "7 - total sulfur dioxide\n",
        "8 - density\n",
        "9 - pH\n",
        "10 - sulphates\n",
        "11 - alcohol\n",
        "```\n",
        "I valori contenuti nel vettore y (contenente Nobs valori) rappresentano un indice di qualità (output feature), che varia tra 0-10. Il task di regressione consiste nella predizione dell'indice di qualità a partire dalle 11 input feature.\n",
        "\n",
        "1.   Caricare il dataset winequality-red.npz, dividerlo in un training (80%) set e test set (20%) e standardizzare i dati impiegando valor medio e deviazione standard del training set\n",
        "2.   Creare una multi-layer perceptron (MLP) con 2 strati nascosti con 64 unità ciascuno (rectified linear units, ReLU come funzione di attivazione).\n",
        "3. Addestrare il modello per 250 epoche impiegando l'ottimizzatore Stochastic Gradient Descent (learning rate=1e-3, batch size=64) e la funzione costo Mean Squared Error. Testare il modello sul test set.\n",
        "4. Visualizzare l'errore valutato sul training set in funzione delle epoche di addestramento.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnanDAeXWrK5"
      },
      "outputs": [],
      "source": [
        "# Prepare the dataset.\n",
        "data = np.load('/content/sample_data/winequality-red.npz') # /path/to/winequality-red.npz\n",
        "x = data['x']\n",
        "y = data['y']\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "plt.hist(y)\n",
        "ntot_examples = x.shape[0]\n",
        "nfeatures = x.shape[-1]\n",
        "nexamples = math.floor(0.8*ntot_examples)\n",
        "\n",
        "print('Number of input features: {0}'.format(nfeatures))\n",
        "print('Number of total examples: {0}'.format(ntot_examples))\n",
        "print('Training set number of examples: {0}'.format(nexamples))\n",
        "print('Test set number of examples: {0}'.format(ntot_examples-nexamples))\n",
        "x_train = x[:nexamples,:]\n",
        "y_train = y[:nexamples]\n",
        "\n",
        "x_test = x[nexamples:, :]\n",
        "y_test = y[nexamples:]\n",
        "\n",
        "mean_train = x_train.mean(axis=0)\n",
        "std_train = x_train.std(axis=0)\n",
        "x_train -= mean_train\n",
        "x_train /= std_train\n",
        "x_test -= mean_train\n",
        "x_test /= std_train\n",
        "\n",
        "# Model design. \n",
        "inputs = keras.Input(shape=(nfeatures,), name=\"input_features\")\n",
        "x1 = layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.L2(1e-4), bias_regularizer=regularizers.L2(1e-4),name='dense_layer0')(inputs)\n",
        "x2 = layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.L2(1e-4), bias_regularizer=regularizers.L2(1e-4),name='dense_layer1')(x1)\n",
        "outputs = layers.Dense(1, name=\"dense_layer_output\")(x2)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name='MLP')\n",
        "model.summary()\n",
        "# Instantiate an optimizer.\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRQ-eOXS_s2-"
      },
      "outputs": [],
      "source": [
        "nepochs = 50#250\n",
        "bs = 64\n",
        "nexamples = x_train.shape[0]\n",
        "\n",
        "idx_start = np.arange(0, x_train.shape[0], bs)\n",
        "idx_stop = np.roll(idx_start, shift=-1)\n",
        "idx_stop[-1] = x_train.shape[0]\n",
        "\n",
        "nbatches = idx_start.shape[0]\n",
        "print('Number of batches ({0} examples, {1} mini-batch size): {2}, first: {3}, last: {4}'.format(nexamples, bs, nbatches, \n",
        "                                                             [idx_start[0], idx_stop[0]],\n",
        "                                                             [idx_start[-1], idx_stop[-1]]))\n",
        "seed = 1234\n",
        "np.random.seed(seed)\n",
        "\n",
        "rmse_fn = tf.keras.metrics.RootMeanSquaredError()\n",
        "mae_fn = tf.keras.metrics.MeanAbsoluteError()\n",
        "# Iterate over epochs.\n",
        "train_losses = []\n",
        "for epoch in np.arange(nepochs):\n",
        "    # Shuffling training data at the start of each epoch\n",
        "    idx = np.arange(x_train.shape[0])\n",
        "    np.random.shuffle(idx)\n",
        "    loss = []\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step in np.arange(nbatches):\n",
        "        # Extracting mini-batch.\n",
        "        idx_batch = idx[idx_start[step]: idx_stop[step]]\n",
        "        x_batch_train = x_train[idx_batch, :]\n",
        "        y_batch_train = y_train[idx_batch]\n",
        "        # Converting numpy arrays to tensorflow tensors.\n",
        "        x_batch_train = tf.convert_to_tensor(x_batch_train)\n",
        "        y_batch_train = tf.convert_to_tensor(y_batch_train)\n",
        "        # Open a GradientTape to record the operations run during the forward-pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Run the forward-pass of the layer.\n",
        "            preds = model(x_batch_train, training=True)\n",
        "            # Compute the loss value for this minibatch.\n",
        "            loss_value = loss_fn(y_batch_train, preds)\n",
        "        # Use the gradient tape to automatically retrieve the gradients of loss with respect to the trainable parameters.\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        # Run one step of gradient descent by updating the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        loss.append(loss_value)\n",
        "    train_loss = np.mean(loss)  # Averaging mini-batch losses\n",
        "    print('Epoch {0}, train loss: {1}'.format(epoch, train_loss))\n",
        "    train_losses.append(train_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NechlZjQWgL"
      },
      "outputs": [],
      "source": [
        "plt.semilogy(np.arange(nepochs), train_losses, 'k')\n",
        "plt.xlabel('Epoche')\n",
        "plt.ylabel('MSE (log)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwfiMWVeQUUy"
      },
      "outputs": [],
      "source": [
        "# Network evaluation at each epoch on both training and test set (alla fine ultima!!)\n",
        "preds = model(x_train, training=False)\n",
        "train_rmse = rmse_fn(preds, tf.convert_to_tensor(y_train))\n",
        "train_mae = mae_fn(preds, tf.convert_to_tensor(y_train))\n",
        "\n",
        "preds = model(x_test, training=False)\n",
        "test_rmse = rmse_fn(preds, tf.convert_to_tensor(y_test))\n",
        "test_mae = mae_fn(preds, tf.convert_to_tensor(y_test))\n",
        "\n",
        "print('Test RMSE: {0}, test MAE: {1}'.format(test_rmse, test_mae))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgwxYKPW9yJ1"
      },
      "outputs": [],
      "source": [
        "# Model design. \n",
        "inputs = keras.Input(shape=(nfeatures,), name=\"input_features\")\n",
        "x1 = layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.L2(1e-4), bias_regularizer=regularizers.L2(1e-4),name='dense_layer0')(inputs)\n",
        "x2 = layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.L2(1e-4), bias_regularizer=regularizers.L2(1e-4),name='dense_layer1')(x1)\n",
        "outputs = layers.Dense(1, name=\"dense_layer_output\")(x2)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name='MLP')\n",
        "model.summary()\n",
        "# Instantiate an optimizer.\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=loss_fn,\n",
        "              metrics=[rmse_fn,mae_fn])\n",
        "model.fit(x=x_train,y=y_train,batch_size=bs,\n",
        "          epochs=50,#250\n",
        "          shuffle=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlBigV4BR9XF"
      },
      "outputs": [],
      "source": [
        "# Network evaluation at each epoch on both training and test set (alla fine ultima!!)\n",
        "preds = model(x_train, training=False)\n",
        "train_rmse = rmse_fn(preds, tf.convert_to_tensor(y_train))\n",
        "train_mae = mae_fn(preds, tf.convert_to_tensor(y_train))\n",
        "\n",
        "preds = model(x_test, training=False)\n",
        "test_rmse = rmse_fn(preds, tf.convert_to_tensor(y_test))\n",
        "test_mae = mae_fn(preds, tf.convert_to_tensor(y_test))\n",
        "\n",
        "print('Test RMSE: {0}, test MAE: {1}'.format(test_rmse, test_mae))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKWy6bbGReMb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
