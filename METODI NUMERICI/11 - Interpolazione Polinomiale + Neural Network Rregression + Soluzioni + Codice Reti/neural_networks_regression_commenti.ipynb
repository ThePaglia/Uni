{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fqs1bZ3q3NKi"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lB19h5MbMUap"
   },
   "source": [
    "# Predizione della qualità di un vino partendo da risultati di test fisico-chimici\n",
    "Il dataset è disponibile pubblicamente online (https://archive.ics.uci.edu/ml/datasets/Wine+Quality) e viene fornito (per rapidità/semplicità) in formato numpy (.npz) per quest'esercitazione (winequality-white.npz e winequality-red.npz). \n",
    "\n",
    "La matrice fornita nella variabile x ha dimensioni Nobs x 11 (vini bianchi: Nobs=4898; vini rossi: Nobs=1599). Le righe rappresentano le diverse osservazioni, mentre le colonne rappresentano le feature:\n",
    "```\n",
    "1 - fixed acidity\n",
    "2 - volatile acidity\n",
    "3 - citric acid\n",
    "4 - residual sugar\n",
    "5 - chlorides\n",
    "6 - free sulfur dioxide\n",
    "7 - total sulfur dioxide\n",
    "8 - density\n",
    "9 - pH\n",
    "10 - sulphates\n",
    "11 - alcohol\n",
    "```\n",
    "I valori contenuti nel vettore y (contenente Nobs valori) rappresentano un indice di qualità (output feature), che varia tra 0-10. Il task di regressione consiste nella predizione dell'indice di qualità a partire dalle 11 input feature.\n",
    "\n",
    "1.   Caricare il dataset winequality-red.npz, dividerlo in un training (80%) set e test set (20%) e standardizzare i dati impiegando valor medio e deviazione standard del training set\n",
    "2.   Creare un multi-layer perceptron (MLP) con 2 strati nascosti con 64 unità ciascuno (rectified linear units, ReLU come funzione di attivazione).\n",
    "3. Addestrare il modello per 250 epoche impiegando l'ottimizzatore Stochastic Gradient Descent (learning rate=1e-3, batch size=64) e la funzione costo Mean Squared Error. Testare il modello sul test set.\n",
    "4. Visualizzare l'errore valutato sul training set in funzione delle epoche di addestramento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqQuZr25exBY"
   },
   "outputs": [],
   "source": [
    "# Prepare the dataset.\n",
    "\n",
    "data = np.load('/content/sample_data/winequality-red.npz') # /path/to/winequality-red.npz\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnanDAeXWrK5"
   },
   "outputs": [],
   "source": [
    "# Prepare the dataset.\n",
    "data = np.load('/content/sample_data/winequality-red.npz') # /path/to/winequality-red.npz\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "plt.hist(y)\n",
    "ntot_examples = x.shape[0]\n",
    "nfeatures = x.shape[-1]\n",
    "nexamples = math.floor(0.8*ntot_examples)\n",
    "\n",
    "print('Number of input features: {0}'.format(nfeatures))\n",
    "print('Number of total examples: {0}'.format(ntot_examples))\n",
    "print('Training set number of examples: {0}'.format(nexamples))\n",
    "print('Test set number of examples: {0}'.format(ntot_examples-nexamples))\n",
    "x_train = x[:nexamples,:]\n",
    "y_train = y[:nexamples]\n",
    "\n",
    "x_test = x[nexamples:, :]\n",
    "y_test = y[nexamples:]\n",
    "\n",
    "mean_train = x_train.mean(axis=0) # osserv. x feature\n",
    "std_train = x_train.std(axis=0)\n",
    "x_train -= mean_train\n",
    "x_train /= std_train\n",
    "x_test -= mean_train\n",
    "x_test /= std_train\n",
    "\n",
    "# Model design.  (keras (tensorflow), tensorflow, pytorch)\n",
    "inputs = keras.Input(shape=(nfeatures,), name=\"input_features\")\n",
    "x1 = layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.L2(1e-4), bias_regularizer=regularizers.L2(1e-4),name='dense_layer0')(inputs)\n",
    "x2 = layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.L2(1e-4), bias_regularizer=regularizers.L2(1e-4),name='dense_layer1')(x1)\n",
    "outputs = layers.Dense(1, name=\"dense_layer_output\")(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='MLP')\n",
    "model.summary()\n",
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9)# \n",
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnCPPEDSrqL9"
   },
   "outputs": [],
   "source": [
    "nepochs = 50#250\n",
    "bs = 64\n",
    "nexamples = x_train.shape[0]\n",
    "\n",
    "idx_start = np.arange(0, x_train.shape[0], bs)\n",
    "idx_stop = np.roll(idx_start, shift=-1)\n",
    "idx_stop[-1] = x_train.shape[0]\n",
    "\n",
    "nbatches = idx_start.shape[0]\n",
    "# x_train: (1279, 11)\n",
    "# epoca 1\n",
    "# mini-batch 1: [0, 64] 64 esempi, 1 iterazione\n",
    "# mini-batch 2: [64, 128] 64 esempi, 2 iterazione\n",
    "# ...\n",
    "# mini-batch 20: [1216, 1279] <64 esempi (1279 non divisibile 64), 20ma iterazione\n",
    "\n",
    "# epoca 2\n",
    "# mini-batch 1: [0, 64] 64 esempi, 1 iterazione\n",
    "#...\n",
    "# mini-batch 20: [1216, 1279] <64 esempi (1279 non divisibile 64), 20ma iterazione\n",
    "#...\n",
    "\n",
    "# epoca 50\n",
    "print('Number of batches ({0} examples, {1} mini-batch size): {2}, first: {3}, last: {4}'.format(nexamples, bs, nbatches, \n",
    "                                                             [idx_start[0], idx_stop[0]],\n",
    "                                                             [idx_start[-1], idx_stop[-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRQ-eOXS_s2-"
   },
   "outputs": [],
   "source": [
    "nepochs = 50#250\n",
    "bs = 64\n",
    "nexamples = x_train.shape[0]\n",
    "\n",
    "idx_start = np.arange(0, x_train.shape[0], bs)\n",
    "idx_stop = np.roll(idx_start, shift=-1)\n",
    "idx_stop[-1] = x_train.shape[0]\n",
    "\n",
    "nbatches = idx_start.shape[0]\n",
    "print('Number of batches ({0} examples, {1} mini-batch size): {2}, first: {3}, last: {4}'.format(nexamples, bs, nbatches, \n",
    "                                                             [idx_start[0], idx_stop[0]],\n",
    "                                                             [idx_start[-1], idx_stop[-1]]))\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Iterate over epochs.\n",
    "train_losses = []\n",
    "train_losses_it = []\n",
    "for epoch in np.arange(nepochs): # epoca 0: epoca 49\n",
    "    # Shuffling training data at the start of each epoch\n",
    "    idx = np.arange(x_train.shape[0]) # idx: [0,1,2,...,1278] valori da 0 a 1279\n",
    "    np.random.shuffle(idx)# idx: [100, 2, 5, 1, 137, 7,...] valori da 0 a 1279\n",
    "    loss = []\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step in np.arange(nbatches): # step 0: step 19\n",
    "        # Extracting mini-batch.\n",
    "        # idx_start[step=0] = 0, idx_stop[step=0] = 64\n",
    "        # idx[idx_start[0]: idx_stop[0]]\n",
    "        idx_batch = idx[idx_start[step]: idx_stop[step]] # \n",
    "        x_batch_train = x_train[idx_batch, :]\n",
    "        y_batch_train = y_train[idx_batch]\n",
    "        # Converting numpy arrays to tensorflow tensors.\n",
    "        x_batch_train = tf.convert_to_tensor(x_batch_train)\n",
    "        y_batch_train = tf.convert_to_tensor(y_batch_train)\n",
    "        # Open a GradientTape to record the operations run during the forward-pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Run the forward-pass of the layer.\n",
    "            preds = model(x_batch_train, training=True)\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(y_batch_train, preds)\n",
    "        # Use the gradient tape to automatically retrieve the gradients of loss with respect to the trainable parameters.\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # Run one step of gradient descent by updating the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        # grads: grad(L/w0), grad(L/w1),..., \n",
    "        # model.trainable_weights: w0, w1,..., \n",
    "        \n",
    "        loss.append(loss_value) # lista di 20\n",
    "    train_losses_it.extend(loss)\n",
    "    train_loss = np.mean(loss)  # Averaging mini-batch losses\n",
    "    print('Epoch {0}, train loss: {1}'.format(epoch, train_loss))\n",
    "    train_losses.append(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NechlZjQWgL"
   },
   "outputs": [],
   "source": [
    "plt.semilogy(np.arange(nepochs*nbatches), train_losses_it, 'k')\n",
    "for i in np.arange(0, nepochs*nbatches, nbatches):\n",
    "    plt.axvline(i, c='r')\n",
    "plt.xlim([0, 10*nbatches])\n",
    "plt.xlabel('Iterazioni')\n",
    "plt.ylabel('MSE (log)')\n",
    "# potete plottare anche la funzione costo su singoli step (singoli mini-batch)\n",
    "# valutare la variabilità (np.std) della funzione costo tra le valutazioni di mini-batch in 1 epoca\n",
    "# variando il mini-batch size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwfiMWVeQUUy"
   },
   "outputs": [],
   "source": [
    "# Network evaluation at each epoch on both training and test set (alla fine ultima!!)\n",
    "rmse_fn = tf.keras.metrics.RootMeanSquaredError()\n",
    "mae_fn = tf.keras.metrics.MeanAbsoluteError()\n",
    "\n",
    "preds = model(x_train, training=False)\n",
    "train_rmse = rmse_fn(preds, tf.convert_to_tensor(y_train))\n",
    "train_mae = mae_fn(preds, tf.convert_to_tensor(y_train))\n",
    "\n",
    "preds = model(x_test, training=False)\n",
    "test_rmse = rmse_fn(preds, tf.convert_to_tensor(y_test))\n",
    "test_mae = mae_fn(preds, tf.convert_to_tensor(y_test))\n",
    "\n",
    "print('Test RMSE: {0}, test MAE: {1}'.format(test_rmse, test_mae))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-scwVi-5xTu"
   },
   "source": [
    "Keras introduce anche un metodo che realizza il ciclo di addestramento come realizzato sopra. Di seguito viene illustrato come eseguire l'addestramento chiamando il metodo 'fit' della classe 'Model' di Keras. Viene anche testato il modello in una cella separata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgwxYKPW9yJ1"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Model design. \n",
    "inputs = keras.Input(shape=(nfeatures,), name=\"input_features\")\n",
    "x1 = layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.L2(1e-4), bias_regularizer=regularizers.L2(1e-4),name='dense_layer0')(inputs)\n",
    "x2 = layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.L2(1e-4), bias_regularizer=regularizers.L2(1e-4),name='dense_layer1')(x1)\n",
    "outputs = layers.Dense(1, name=\"dense_layer_output\")(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='MLP')\n",
    "model.summary()\n",
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss_fn,\n",
    "              metrics=[rmse_fn,mae_fn])\n",
    "model.fit(x=x_train,y=y_train,batch_size=bs,\n",
    "          epochs=50,#250\n",
    "          shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlBigV4BR9XF"
   },
   "outputs": [],
   "source": [
    "# Network evaluation at each epoch on both training and test set (alla fine ultima!!)\n",
    "preds = model(x_train, training=False)\n",
    "train_rmse = rmse_fn(preds, tf.convert_to_tensor(y_train))\n",
    "train_mae = mae_fn(preds, tf.convert_to_tensor(y_train))\n",
    "\n",
    "preds = model(x_test, training=False)\n",
    "test_rmse = rmse_fn(preds, tf.convert_to_tensor(y_test))\n",
    "test_mae = mae_fn(preds, tf.convert_to_tensor(y_test))\n",
    "\n",
    "print('Test RMSE: {0}, test MAE: {1}'.format(test_rmse, test_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1ARrQa66UuN"
   },
   "source": [
    "Ci sono alcuni parametri del modello o parametri della procedura di addestramento che possono influenzare la qualità della predizione del modello (cosiddetti 'iper-parametri'), come:\n",
    "*   numero di strati nascosti (2, 3, 4, 5)\n",
    "*   numero di unità nascoste (16, 32, 64, 128)\n",
    "*   funzione di attivazione delle unità nascoste ('relu', 'elu')\n",
    "*   inclusione di 'dropout' nella rete, dopo uno strato nascosto\n",
    "*   tipologia di ottimizzatore ('SGD', 'SGD' con parametro momentum=0.9, 'Adam', 'RMSprop')\n",
    "*   learning rate (1e-4, 5*1e-3, 1e-3)\n",
    "*   mini-batch size (16, 32, 64)\n",
    "*   numero di epoche di addestramento (100, 150, 200, 250, 300)\n",
    "\n",
    "Questi sono solo alcuni dei principali iper-parametri introdotti. Fra parentesi sono indicati alcuni valori che potete testare (non sono necessari tutti) per vedere l'effetto di essi sulle metriche di performance. \n",
    "\n",
    "Addestrate il modello cambiando un iper-parametro alla volta (es. numero di strati nascosti=3 invece di 2 (default)) e registrate le metriche di prestazione che risultano da ogni modello addestrato. \n",
    "\n",
    "----30-40'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4UgB30D5r4p"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
