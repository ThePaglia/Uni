{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Esercizio 1. Predizione della qualità di un vino partendo da risultati di test fisico-chimici mediante percettrone multi-strato\n",
        "Con riferimento all'esercizio tutorial descritto in 'neural_networks_regression.ipynb', risolvere il problema affrontato con regressione mediante classificazione. \n",
        "\n",
        "Per fare ciò occorre modificare:\n",
        "\n",
        "*   encoding delle etichette (one-hot encoding, vedi sotto), sia in fase di training sia di testing \n",
        "*   lo strato di uscita con un neurone per classe (11 in tot), attivati mediante softmax (activation='softmax')\n",
        "*   funzione costo (tf.keras.losses.CategoricalCrossentropy())\n",
        "*   metriche di performance (tf.keras.metrics.CategoricalAccuracy())\n",
        "*   (class weighting per dataset sbilanciati)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AdUSfqaRK03m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = [1, 3, 7, 1, 0, ..., 9] # 11 features\n",
        "yvera = 4 # dense label coding\n",
        "yp = [0.1, 0., 0.2, 0.6, ..., ] # probabilità: sommano ad 1, supponiamo alla prima iterazione\n",
        "yp_ = [0.0001, 0.001, 0.00002, 0.89, ..., ]# all'ultima iterazione\n",
        "\n",
        "# ciascun valore mi dice la probabilità che l'ingresso X appartenga ad una certa classe \n",
        "yvera_prb = [0, 0, 0, 1, 0, 0, ... 0] # (one-hot encoding) vettori a 11 dimen.\n",
        "# minimizzare la cross-entropy tra yvera_prb, yp equivale a minimizzare la\n",
        "# distanza tra le distribuzioni di probabilità. Voi le potete vedere come distanze vettoriali\n",
        "# tra il vettore ad 11 dim yp e vettore yvera_prb\n",
        "\n",
        "# distanza tra distr. proba: distranza di Kullback-leibler \n",
        "\n",
        "y_train # (1300,) [0, 3, 7, 1, 5, 9,...]\n",
        "y_train_onehot # (1300, 11), dove ciascuna riga ha la versione one-hot dell'etichetta associata\n",
        "y_test\n",
        "y_test_onehot\n",
        "\n",
        "# acc: no. esempi correttamente classificati / no. totale di esempi\n",
        "# confusion matrix (matrice di confusione), da questa vengono poi definite tante metriche di prestazione (acc, F1 score,...)\n",
        "# esistono altre metriche speicifhe per dataset sbilanciati come F1 score"
      ],
      "metadata": {
        "id": "9ovBWtu6rcLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "# one-hot encoding\n",
        "# supponendo di aver caricato y_train e y_test...\n",
        "y_train_onehot = np.zeros((y_train.shape[0], 11))\n",
        "y_train_onehot[np.arange(y_train.shape[0]), y_train] = 1\n",
        "\n",
        "y_test_onehot = np.zeros((y_test.shape[0], 11))\n",
        "y_test_onehot[np.arange(y_test.shape[0]), y_test] = 1"
      ],
      "metadata": {
        "id": "UXy0dC9VLStK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Esercizio 2. Classificazione di caratteri scritti a mano su carta (handwritten digit classification) mediante rete neurale convoluzionale\n",
        "\n",
        "Il compito di classificazione obiettivo consiste nell'associare l'etichetta corretta a numeri scritti a mano riportati in un'immagine gray-scale di dimensioni 28x28. Quindi, essendoci 10 possibili digit (0,...,9) il compito consiste nella discriminazione di 10 possibili classi.\n",
        "\n",
        "Il dataset è disponibile pubblicamente online (http://yann.lecun.com/exdb/mnist/) ma utilizzeremo le funzioni di keras per caricarlo in memoria (vedi sotto). \n",
        "\n",
        "Utilizzare una rete neurale convoluzionale (CNN) per risolvere il problema di classificazione. Definire la seguente sequenza di strati:\n",
        "\n",
        "\n",
        "INPUT-CONV-MAXPOOL-DROPOUT-CONV-MAXPOOL-DROPOUT-FLATTEN-DENSE(OUTPUT)\n",
        "\n",
        "\n",
        "Definire gli strati intermedi con: \n",
        " \n",
        "*   Conv2D(16, kernel_size=(5, 5), in cui viene eseguita la convoluzione con 16 filtri convoluzionali addestrabili 5x5\n",
        "*   MaxPooling2D(pool_size=(2, 2), in cui viene eseguito l'operatore di max pooling per dimezzare le dimensioni spaziali (ridurre computazioni)\n",
        "*   Dropout(0.25), operatore di dropout (dropout rate=0.25)\n",
        "*   Flatten()\n",
        "\n",
        "Vedete anche i seguenti link per comprendere meglio il funzionamento di CNN per classificazione di digits:\n",
        "https://adamharley.com/nn_vis/cnn/2d.html.\n",
        "\n",
        "Addestrare e testare il modello per risolvere il problema di classificazione, analogamente a quanto fatto per l'esercizio 1. \n",
        "\n",
        "N.B. visto che ci sono molti esempi, valutare di addestrare per poche epoche (es. fino 50) con mini-batch size elevati (es. 128 o 256)."
      ],
      "metadata": {
        "id": "xeTtUHcxLxHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "# Prepare the dataset.\n",
        "# load the data and split it between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() # uint8 (0-255)\n",
        "N = 28 # number of rows and columns of the input square matrix\n",
        "C = 10 # number of total classes (e.g., 10 possible digits)\n",
        "\n",
        "print(\"Input data type:\", x_train.dtype)\n",
        "print(\"Min value:\", x_train.min())\n",
        "print(\"Max value:\", x_train.max())\n",
        "print(\"Shape of the training examples:\", x_train.shape)\n",
        "print(\"Shape of the test examples:\", x_test.shape)\n",
        "\n",
        "plt.hist(y_train)\n",
        "plt.show()\n",
        "\n",
        "y_train_onehot = np.zeros((y_train.shape[0], 10))\n",
        "y_train_onehot[np.arange(y_train.shape[0]), y_train] = 1\n",
        "\n",
        "y_test_onehot = np.zeros((y_test.shape[0], 10))\n",
        "y_test_onehot[np.arange(y_test.shape[0]), y_test] = 1\n",
        "print(y_test[0])\n",
        "print(y_test_onehot[0,:])\n",
        "\n",
        "mean_train = x_train.mean(axis=0)\n",
        "std_train = x_train.std(axis=0)\n",
        "x_train = x_train-mean_train\n",
        "x_train = x_train/(std_train+1e-16)\n",
        "x_test = x_test-mean_train\n",
        "x_test = x_test/(std_train+1e-16)\n",
        "\n",
        "\n",
        "input_shape = (N, N, 1) # input dimension (grey-scale image 28 x 28, represented as a single input feature map 28x28x1)\n",
        "inputs = keras.Input(shape=input_shape, name=\"input_features\")\n",
        "x1 = layers.Conv2D(16, kernel_size=(5, 5),  activation=\"relu\",\n",
        "                   kernel_regularizer=regularizers.L2(1e-4),\n",
        "                   bias_regularizer=regularizers.L2(1e-4),\n",
        "                   name='conv_layer0')(inputs)\n",
        "x2 = layers.MaxPooling2D(pool_size=(2, 2),name='maxpool0')(x1)\n",
        "x3 = layers.Dropout(0.25,name='dropout0')(x2)\n",
        "\n",
        "x4 = layers.Conv2D(16, kernel_size=(5, 5),activation=\"relu\",\n",
        "                   kernel_regularizer=regularizers.L2(1e-4),\n",
        "                   bias_regularizer=regularizers.L2(1e-4),\n",
        "                   name='conv_layer1')(x3)\n",
        "x5 = layers.MaxPooling2D(pool_size=(2, 2),name='maxpool1')(x4)\n",
        "x6 = layers.Dropout(0.25,name='dropout1')(x5)\n",
        "\n",
        "x7 = layers.Flatten()(x6)\n",
        "outputs = layers.Dense(C, activation='softmax', name='dense0')(x7)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name='CNN')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "kU313g6sNTeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IJGcUEipOeOb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}